{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week11_ImageClassification",
      "provenance": [],
      "authorship_tag": "ABX9TyPJQUF4jdbQdfqvhmey/MCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch00226855/CMP414765Fall2022/blob/main/Week11_ImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwszdj54lJYI"
      },
      "source": [
        "# Week 11\n",
        "# Image Classification with Convolutional Neural Network (CNN)\n",
        "\n",
        "**Reference:** [TensorFlow Tutorial on Convolutional Neural Networks](https://www.tensorflow.org/tutorials/images/cnn)\n",
        "\n",
        "**Please enable GPU computing before proceed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlP21Dd3lUmQ"
      },
      "source": [
        "### Ideas\n",
        "- Dense layers may contain redudent connections\n",
        "- Some information should be invariant to spacial translation\n",
        "- The number of parameters can be reduced if certain weights share the same value.\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcS4LZdFg5QPbgDb-jvP-YT0N51eRkWg45uF0ybsB5k0Ubr0-gOC&usqp=CAU\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bUdECNflUqJ"
      },
      "source": [
        "## 2D Convolution Layer\n",
        "<img src=\"https://cdn-media-1.freecodecamp.org/images/Gjxh-aApWTzIRI1UNmGnNLrk8OKsQaf2tlDu\" width=\"600\">\n",
        "\n",
        "**Comparison:**\n",
        "If this were a densely connect layer:\n",
        "- The number of connection would be: 64 * 64 = 4096\n",
        "- The number of weight parameters would be: 4096\n",
        "\n",
        "Now that this is a convolutional layer (with a 3*3 filter):\n",
        "- The number of connection is: 9 * 64 = 576\n",
        "- The number of parameters is: 9\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwW4aozplUsh"
      },
      "source": [
        "**2D smoothing with Gaussian kernel**\n",
        "\n",
        "<img src=\"https://www.cs.umd.edu/class/fall2016/cmsc426/matlab/filters/html/filters_tutorial_03.png\" width=\"600\">\n",
        "\n",
        "<img src=\"https://www.mathworks.com/help/examples/stats/win64/ComputeTheMultivariateNormalPdfExample_01.png\" width=\"400\">\n",
        "\n",
        "**Edge detection**\n",
        "\n",
        "<img src=\"https://media5.datahacker.rs/2018/10/edges.jpg\" width=\"600\">\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Ching-Wei_Wang/publication/221472523/figure/fig5/AS:305540338077700@1449857901164/Convolution-filter-for-simple-edge-detect.png\" width=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLjGZkvdlUuv"
      },
      "source": [
        "## LeNet5 on MNIST\n",
        "\n",
        "Yann LeCun, Leon Bottou, Yosuha Bengio and Patrick Haffner proposed a neural network architecture for handwritten and machine-printed character recognition in 1990â€™s which they called LeNet-5. It is one of the early example of a convolutional neural network\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBDLWUMXlUxd"
      },
      "source": [
        "## Max-Pooling Layer\n",
        "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM3NwmG7lUz5"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK6gaIZllU22"
      },
      "source": [
        "# Load and prepare the MNIST dataset.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Convert the data from integers to floating-point numbers\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mlo6L7WlU4_"
      },
      "source": [
        "model_cnn = tf.keras.models.Sequential()\n",
        "\n",
        "model_cnn.add(tf.keras.layers.Conv2D(filters=6,\n",
        "                                 kernel_size=(3, 3),\n",
        "                                 activation='relu',\n",
        "                                 input_shape=(28, 28, 1)))\n",
        "\n",
        "model_cnn.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model_cnn.add(tf.keras.layers.Conv2D(filters=16,\n",
        "                                 kernel_size=(3, 3),\n",
        "                                 activation='relu'))\n",
        "\n",
        "model_cnn.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "model_cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model_cnn.add(tf.keras.layers.Dense(units=120,\n",
        "                       activation='relu'))\n",
        "model_cnn.add(tf.keras.layers.Dense(units=84,\n",
        "                       activation='relu'))\n",
        "model_cnn.add(tf.keras.layers.Dense(units=10,\n",
        "                       activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k4KnhA7lU7L"
      },
      "source": [
        "model_cnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un9uUBdPlU9z"
      },
      "source": [
        "model_cnn.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # (from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJPG5htRlVAI"
      },
      "source": [
        "model_cnn.fit(x_train.reshape(list(x_train.shape) + [1]), y_train, epochs=10, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDHmNAYTlVC2"
      },
      "source": [
        "model_cnn.evaluate(x_test.reshape(list(x_test.shape) + [1]), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82mo08SBlVFg"
      },
      "source": [
        "## Dropout and Model Regularization\n",
        "For a complicated model like deep neural networks, a major concern on its performance is model overfitting:\n",
        "\n",
        "![underfitting and overfitting](https://cdn-images-1.medium.com/max/1200/1*cdvfzvpkJkUudDEryFtCnA.png)\n",
        "\n",
        "In plain words, overfitting happens when the model is **memorizing** the training data, and become poorly at **generalizing** what they've learned to unseen data. Think about a student who memorized the entire machine learning textbook. He may appear quite knowledgable in machine learning when asked things directly from the book, but there is no way he can perform a machine project on a dataset not mentioned in the book.\n",
        "\n",
        "### How to dentify model overfitting?\n",
        "- Visualize the model (decision boundary, regression curves, etc.)\n",
        "- Observe the trends in training loss and the testing loss\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*vuZxFMi5fODz2OEcpG-S1g.png)\n",
        "\n",
        "### How to prevent model overfitting?\n",
        "1. Start with a simple model\n",
        "\n",
        "![](https://image.slidesharecdn.com/lawsofwebdesign-091104020153-phpapp01/95/laws-of-web-development-11-728.jpg?cb=1257384621)\n",
        "2. Add penalty to complicated models\n",
        "    - L1 Regularizor\n",
        "    - L2 Regularizor\n",
        "    - Elastic Net\n",
        "\n",
        "3. (For Neural Networks) Dropout layers: remove weights to the next layer\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1800/1*iWQzxhVlvadk6VAJjsgXgg.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7C9uja3lVIo"
      },
      "source": [
        "# Image Classification with CIFAR-10 Dataset\n",
        "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is a widely used benchmark dataset for image classifiers. The dataset consists of 10 classes of color images of size $32\\times 32$. Let's build a neural network with **convolutional layers** to classify the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfGOVAjClVLW"
      },
      "source": [
        "### Download the dataset\n",
        "- Use `request` to download the tar file from [https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)\n",
        "- Use `tarfile` to extract files\n",
        "- Use `pickle` to load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvd_rMKylVOV"
      },
      "source": [
        "import requests\n",
        "\n",
        "filename = \"cifar-10-python.tar.gz\"\n",
        "if not os.path.isfile(filename):\n",
        "    print(\"Downloading CIFAR10 dataset...\")\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    file = requests.get(url)\n",
        "\n",
        "    print(\"Writing to file\", filename, \"...\")\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(file.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG81Rh39njsY"
      },
      "source": [
        "import tarfile\n",
        "datapath = \"cifar-10-batches-py/\" \n",
        "if not os.path.isdir(datapath):\n",
        "    print(\"Extracting files...\")\n",
        "    tar = tarfile.open(filename)\n",
        "    tar.extractall()\n",
        "    print(\"Files extracted.\")\n",
        "    tar.close()\n",
        "os.listdir(datapath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4qtN_Kwnkz2"
      },
      "source": [
        "# load one batch\n",
        "import pickle\n",
        "with open(datapath + \"data_batch_1\", \"rb\") as f:\n",
        "    batch = pickle.load(f, encoding=\"latin1\")\n",
        "    features = batch['data'].reshape([len(batch['data']), 3, 32, 32]).transpose(0, 2, 3, 1)\n",
        "    labels = batch['labels']\n",
        "print(\"feature size:\", features.shape)\n",
        "print(\"label size:\", len(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-O9uujRnmGr"
      },
      "source": [
        "The label data is just a list of 10000 numbers in the range 0-9, which corresponds to each of the 10 classes in CIFAR-10. \n",
        "\n",
        "* **airplane**\n",
        "* **automobile**\n",
        "* **bird**\n",
        "* **cat**\n",
        "* **deer**\n",
        "* **dog**\n",
        "* **frog**\n",
        "* **horse**\n",
        "* **ship**\n",
        "* **truck**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbvXzwOgnn7y"
      },
      "source": [
        "# Show a sample image\n",
        "sample_id = 4321\n",
        "plt.imshow(features[sample_id])\n",
        "label_names = ['airplane', 'automobile', 'bird',\n",
        "            'cat', 'deer', 'dog', 'frog',\n",
        "            'horse', 'ship', 'truck']\n",
        "plt.xlabel(label_names[labels[sample_id]])\n",
        "# labels[sample_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_2nNz8CnpN6"
      },
      "source": [
        "# Load all images from batch 1-5\n",
        "train_features = np.empty([0, 32, 32, 3], dtype=np.uint8)\n",
        "train_labels = np.empty([0])\n",
        "for k in range(1, 6):\n",
        "    with open(datapath + \"data_batch_\" + str(k), \"rb\") as f:\n",
        "        batch = pickle.load(f, encoding=\"latin1\")\n",
        "        features = batch[\"data\"].reshape([len(batch['data']), 3, 32, 32]).transpose(0, 2, 3, 1)\n",
        "        labels=batch['labels']\n",
        "        print(\"features shape:\", features.shape)\n",
        "        print(\"labels shape:\", len(labels))\n",
        "        train_features = np.append(train_features, features, axis=0)\n",
        "        train_labels = np.append(train_labels, labels, axis=0)\n",
        "print(\"train_features shape:\", train_features.shape)\n",
        "print(\"train_labels shape:\", train_labels.shape)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xay2mi6LnrEq"
      },
      "source": [
        "## Build CNN model\n",
        "### Create Convolutional Model\n",
        "\n",
        "The entire model consists of 14 layers in total. In addition to layers below lists what techniques are applied to build the model.\n",
        "\n",
        "1. Convolution with 32 different filters in size of (3x3)\n",
        "    - ReLU activation function \n",
        "2. Convolution with 32 different filters in size of (3x3)\n",
        "    - ReLU activation function \n",
        "    - Max Pooling by 2\n",
        "    - Dropout \n",
        "3. Convolution with 64 different filters in size of (3x3)\n",
        "    - ReLU activation function \n",
        "4. Convolution with 64 different filters in size of (3x3)\n",
        "    - ReLU activation function \n",
        "    - Max Pooling by 2\n",
        "    - Dropout \n",
        "5. Flattening the 3-D output of the last convolutional operations.\n",
        "6. Fully Connected Layer with 512 units\n",
        "  - Dropout \n",
        "7. Fully Connected Layer with 10 units (number of image classes)\n",
        "\n",
        "the image below decribes how the conceptual convolving operation differs from the tensorflow implementation when you use [Channel x Width x Height] tensor format. \n",
        "\n",
        "<img src=\"https://adeshpande3.github.io/assets/Cover.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo9fxr9Vntda"
      },
      "source": [
        "batch_size = 32 # How many images to load at a time\n",
        "num_classes = 10 \n",
        "epochs = 10\n",
        "num_predictions = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKz3Lr-3aSxq"
      },
      "source": [
        "# The number of training iterations:\n",
        "50000 / 32 * 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtFtokyrnvob"
      },
      "source": [
        "# Build CNN model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",\n",
        "                              input_shape=features[0].shape, # (32, 32, 3)\n",
        "                              activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",\n",
        "                              activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",\n",
        "                              activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",\n",
        "                              activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "# model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
        "model.add(tf.keras.layers.Dense(num_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rYxSeG_dBft"
      },
      "source": [
        "# How can I print the list of layers?\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69mjsLBtnwrM"
      },
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJxxUhQ8nx3h"
      },
      "source": [
        "# Normalize data\n",
        "def normalize(x):\n",
        "    \"\"\"\n",
        "        argument\n",
        "            - x: input image data in numpy array [32, 32, 3]\n",
        "        return\n",
        "            - normalized x \n",
        "    \"\"\"\n",
        "    min_val = np.min(x)\n",
        "    max_val = np.max(x)\n",
        "    x = (x-min_val) / (max_val-min_val)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGj02QVnnze3"
      },
      "source": [
        "train_features_scaled = normalize(train_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_euMmmhRn0u6"
      },
      "source": [
        "history =  model.fit(train_features_scaled, train_labels, epochs=10, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViXUTJNRn1o7"
      },
      "source": [
        "# Evaluate the model\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy') # allocate validation data to get val_accuracy\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N3j3Jxsn3Tr"
      },
      "source": [
        "# Load test images\n",
        "with open(datapath + \"test_batch\", \"rb\") as f:\n",
        "    batch = pickle.load(f, encoding=\"latin1\")\n",
        "    test_features = batch['data'].reshape([len(batch['data']), 3, 32, 32]).transpose(0, 2, 3, 1)\n",
        "    test_labels = batch['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob0tEPxnn4qT"
      },
      "source": [
        "# Normalize test features\n",
        "test_features_scaled = normalize(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0l6WS3gn5nX"
      },
      "source": [
        "test_labels = np.array(test_labels)\n",
        "test_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrK66WvJn6ig"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_features_scaled,  test_labels, verbose=2)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "test_predictions = model(test_features_scaled)"
      ],
      "metadata": {
        "id": "q9cUtAQmjsK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape:\", test_predictions.shape)\n",
        "print(\"First prediction (logits):\", test_predictions[0])\n",
        "print(\"First prediction (probabilities):\", tf.nn.softmax(test_predictions[0]))\n",
        "print(\"First prediction (class):\", np.argmax(tf.nn.softmax(test_predictions[0])))\n",
        "# confusion_matrix(test_labels, )"
      ],
      "metadata": {
        "id": "I1pMeJ-_kNUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9nTByB0UlwsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions_labels = []\n",
        "for ind in range(10000):\n",
        "    pred = np.argmax(tf.nn.softmax(test_predictions[ind]))\n",
        "    test_predictions_labels.append(pred)\n",
        "print(test_predictions_labels[:10])"
      ],
      "metadata": {
        "id": "_bV-tDQXlLMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test_labels, test_predictions_labels)"
      ],
      "metadata": {
        "id": "Mi_2hzMpl7tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tPiRhSCn7mN"
      },
      "source": [
        "## Addition Discussion: What if we train a dense neural network instead?\n",
        "\n",
        "- Mimic the approach we took in the first neural network example.\n",
        "- Be aware that each color image is represented as a 3D array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-inbVVTgw6c"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}